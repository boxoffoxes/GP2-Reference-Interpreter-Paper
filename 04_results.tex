\section{Performance Evaluation}
\label{sec:performanceevaluation}

In this section we will look at how efficiently our interpreter executes the benchmark programs described in section \ref{sec:benchmark}, and discuss the factors that affect its performance.

While not tuned for speed, the interpreter must still run fast enough to allow its use as a tool to test the correctness of future compiled implementations of the language.


% TODO: mention performance of previous versions, and how increased performance brought bugs and limitations to light.


\subsection{The Test Environment}

We compiled the interpreter using The Glasgow Haskell Compiler version 7.6.3 with optimisations and profiling support enabled:

\begin{verbatim}
$ ghc -O2 -prof -fprof-auto -rtsopts -o gp2 Main.hs
\end{verbatim}

All figures reported here were obtained using a quad-core Intel i7 clocked at 3.4GHz, with 8GB RAM, running 64-bit Ubuntu 14.04 LTS with kernel 3.13.0. The number of processor cores should not have a significant effect on the measured performance of the single-threaded GP~2 interpreter.

We ran benchmarks using the following command:

\begin{verbatim}
$ timeout --foreground 5m time \
      gp2 +RTS -p -sgc.prof -RTS $GPOPT $PROG $GRAPH 10000
\end{verbatim}

The \texttt{timeout} command limits execution time to five minutes for each program/host-graph pair. We used the sum of user and system time reported by the standard Unix \texttt{time} utility as our measure of execution time.

\texttt{+RTS -p -sgc.prof -RTS} 

The arguments to \texttt{gp2} between \texttt{+RTS} and \texttt{-RTS} tell the Haskell run-time system to save profiling information to a file called \texttt{gc.prof} in the current directory, while the \texttt{\$GPOPT} variable was either set to \texttt{--one} to put the interpreter into single-result mode (see table \ref{table:resultsSingle}), or unset for all-result mode (table \ref{table:resultsAll}.

The final three mandatory arguments to the \texttt{gp2} executable specify the benchmark program, the host graph, and the maximum number of rule applications respectively. This last parameter lets us restrict the size of the search space the interpreter explores, but in this case we set the rule application bound large enough that it would not be reached.

We captured the standard-output and standard-error channels from all programs to a file.

\subsection{Host Graphs}
\label{subsec:hosts}

The names of host graphs used for benchmarking give an indication of their structure.

\begin{description}
	\setlength\itemsep{-0.2em}
	\item[Gen $n$] The \textit{Sierpinski} program expects a host graph containing a single node with a numeric label, which controls the number of iterations of the \texttt{expand!} rule to run.

	\item[Linear $n$] A chain of $n$ nodes. The first node has only a single outgoing edge. The last node has only a single incoming edge, and all other nodes have exactly one incoming and one outgoing edge.

	\item[Cyclic $n$] As Linear $n$, but with an extra edge linking the first and final placed nodes so that every node has exactly one incoming and one outgoing edge.

	\item[$x \times y$ Grid] A square lattice of $x$ nodes wide by $y$ nodes tall.
\end{description}

The \textit{shortest distances} benchmark requires all edges to have an integer ``cost'' of traversal. For each host graph passed to this program, we assigned a cost of one to half of the edges, and a cost of two to the other half.



\subsection{Benchmark performance}\label{sec:benchperf}

Tables \ref{table:resultsSingle} and \ref{table:resultsAll} summarise the reference interpreter performance for the five benchmark programs, each run on several host graphs. Table \ref{table:resultsAll} shows figures for generating all possible results for each program/host-graph pair, while table \ref{table:resultsSingle} shows results when the interpreter is running in single output graph mode.


% TODO: noteworthy in single result:
% Acyclicity test on cyclic graphs: 0 rule apps
% Rapid increase in heap usage for Trans.
% 
% noteworthy in all results:
% vertex colouring a 3x3 grid uses in excess of 7 gigabytes of RAM!
% Some benchmarks incur a very low overhead for calculating all compared to single -- Haskell sharing of data structures.
% Shortest distances -- 4x4 was $<0.01$ seconds in single but $>5m$ in all mode
%
% general comments:
% allocd vs live heap -- Haskell requests memory from the OS in large chunks, so allocated can be several times live
% 


\begin{table}[h]
\begin{minipage}{\textwidth}
\centering

\begin{tabular}{llrrcrr}
\hline 
&  & & & & \multicolumn{2}{c}{Heap/kB}\\
Benchmark          & Host Graph & Apps & Time/s   & & Allocd & Live \\
\hline 
\input{results_table_one}
\end{tabular}

\caption[Reference interpreter benchmarks]{Reference interpreter benchmark results when generating a single output graph}

\label{table:resultsSingle}
\end{minipage}
\end{table}



\begin{table}[h]
\begin{minipage}{\textwidth}
\centering

\begin{tabular}{llrrrrrcrr}
\hline 
&  & \multicolumn{3}{c}{Output Graphs} & & && \multicolumn{2}{c}{Heap/kB}\\
Benchmark          & Host Graph & Total & Unique   & Failed & Apps & Time/s   & & Total  & Live \\
\hline 
\input{results_table_all}
\end{tabular}

\caption[Reference interpreter benchmarks]{Reference interpreter benchmark results when generating all possible output graphs}

\label{table:resultsAll}
\end{minipage}
\end{table}


Comparing tables \ref{table:resultsAll} and \ref{table:resultsSingle} gives an indication of the additional costs associated with generating all graphs and isomorphism-checking the results.


% Generating all possible intermediate graphs

%\subsubsection*{Exhaustive search}

When viewing the results in \ref{table:resultsAll}, it should be remembered that the interpreter is computing \textit{all possible} output graphs for a given program/host-graph pair, then consolidating the results based on isomorphism. It is therefore unsurprising that execution time increases exponentially with increasing size of host graph.

However, as already mentioned, more consideration was given to code-correctness and readability than to execution speed or memory consumption.

While performance is acceptable for some quite complex programs, it is nevertheless easy to find cases in which performance rapidly degrades to an unacceptable degree. A particular example being the vertex-colouring benchmark program, which exhibits exponential growth in the number of possible intermediate graphs with increasing edge counts.

One of the most serious limits on the performance of the reference interpreter, which impacts matching of any non-trivial rule, is the naive node-matching strategy. Given a rule with $n$ nodes, select all permutations of $n$ nodes from the host graph, then discard those that fail to meet rootedness, label and edge constraints.

While from the point of view of simplicity this is a reasonable choice for finding all possible outputs for each rule, it is far from an optimal way of finding a single output graph; for any non-trivial rule and host pair the chances of $n$ nodes selected essentially at random matching a rule is very low, meaning that many permutations need to be inspected for each successful match.

That this strategy is repeated for all of the intermediate graphs generated by the previous rule application explains the combinatorial explosions we see in heavily non-deterministic programs like \textit{Sierpinski} and \textit{Shortest distances}.

% interpreter cost vs compiler
%\subsubsection*{Interpreter costs}

Any interpreter has run-time costs which would be paid at compile-time in a compiler. These costs can account for a significant percentage of the execution time.

In the sub-second execution time benchmarks, profiling information indicated that a significant portion of the execution time was spent parsing the rule-graphs and building map structures.


%\subsubsection*{Separate graph and edge matching}

As discussed in section \ref{sec:graph-match} the reference interpreter matches nodes and edges in separate passes. This makes for a simple algorithm at the expense of performance.

A more performance focussed implementation might use a \textit{search plan}\cite{Horvath-Varro07} in which a graph morphism is built incrementally by adding nodes and edges to an existing partial morphism, back-tracking if no suitable candidate can be found.


%\subsubsection*{Unoptimised data structures}

The current version of the interpreter uses a generic lazy map data structure which gives a good balance between cost of search and cost of update. 

Our first prototypes stored the graphs as linked-lists of key/value pairs. The result was an interpreter which spent most of its execution time traversing these lists retrieving nodes and edges. Switching to the faster map structure alone yielded a factor of two speed improvement. A smaller improvement was realised by switching edge keys from an integer to a triplet incorporating the unique identifiers of the source and target nodes.

Nevertheless, an underlying data structure that was tuned to our specific usage patterns and judiciously indexed could dramatically reduce this high cost of node and edge retrieval.

%TODO: not sure this goes here
The incremental improvements we made to our original prototype resulted in a total speed-up of approximately a factor of four hundred! This increase in speed was itself a useful debugging tool, as it permitted running of larger host graphs through our programs, putting greater stress on the stack and heap.

%\subsubsection*{Isomorphism checking}

Another cost associated with generating all possible output graphs is isomorphism checking. In benchmarks where nondeterminism gives us multiple ways of reaching similar output graphs we want a way of reducing this potentially large set of outputs to those which are truly distinct. A particularly extreme example of this can be seen in the \textit{3x3 grid} host graph of the \textit{Shortest distances} benchmark. Here we produced nearly 30~000 isomorphic output graphs! Presumably if allowed to run to completion the \textit{4x4 grid} would yield several orders of magnitude more.


% Cost of profiling
%\subsubsection*{Profiling costs}

We have also incurred some incidental costs by gathering the profiling information used to generate 

% TODO: guestimate profiling costs


% TODO: fail early and fail cheaply!





An unfortunate property of the Haskell memory profiler is that memory profiling information is not saved to disk if the running program is killed by an external process such as the \texttt{timeout} command, which means we are unable to give accurate figures for memory usage for benchmarks which did not complete within the allotted time.


