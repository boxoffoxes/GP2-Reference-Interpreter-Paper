\section{Performance Evaluation}
\label{sec:performanceevaluation}

In this section we will look at how efficiently our interpreter executes the benchmark programs described in Section \ref{sec:benchmark}, and discuss the factors that affect its performance. Though not tuned for speed, the interpreter must run fast enough to allow its use as a practical tool.


% TODO: mention performance of previous versions, and how increased performance brought bugs and limitations to light.


\subsection{The Test Environment}

We compiled the interpreter using The Glasgow Haskell Compiler\cite{ghc} version 7.6.3 with optimisations and profiling support enabled:

\begin{verbatim}
$ ghc -O2 -prof -fprof-auto -rtsopts -o gp2 Main.hs
\end{verbatim}

All figures reported were obtained using a quad-core Intel i7 clocked at 3.4GHz, with 8GB RAM, running 64-bit Ubuntu 14.04 LTS with kernel 3.13.0. The number of processor cores should not have a significant effect on the measured performance of the single-threaded GP~2 interpreter.

We ran benchmarks using the following command

\begin{verbatim}
$ timeout --foreground 5m time \
      gp2 +RTS -p -sgc.prof -RTS $GPOPT $PROG $GRAPH 10000
\end{verbatim}

\noindent
limiting execution time to five minutes for each application of a program to a host graph. We used the sum of user and system time reported by the standard \texttt{time} utility as our measure of execution time.
The arguments to \texttt{gp2} between \texttt{+RTS} and \texttt{-RTS} tell the Haskell run-time system to save profiling information.  The \texttt{\$GPOPT} variable was either set to \texttt{--one} to put the interpreter into single-result mode (see Table \ref{table:resultsSingle}), or unset for all-result mode (see Table \ref{table:resultsAll}).
The final three mandatory arguments to the \texttt{gp2} executable specify the benchmark program, the host graph, and the maximum number of rule applications, as described in Section \ref{sec:implementation}.

\subsection{Host Graphs}
\label{subsec:hosts}

The names of host graphs used for benchmarking give an indication of their structure.

\vspace{.5\baselineskip}
\noindent
\emph{Gen $n$}. The \textit{Sierpinski} program expects a host graph containing a single node with a numeric label, which controls the number of iterations of the \texttt{expand!} command.

\vspace{.5\baselineskip}
\noindent
\emph{Linear $n$}. A chain of $n$ nodes. The first node has only a single outgoing edge. The last node has only a single incoming edge. All other nodes have exactly one incoming and one outgoing edge.

\vspace{.5\baselineskip}
\noindent
\emph{Cyclic $n$}. As Linear $n$, but with an extra edge from the last node to the first, so every node has exactly one incoming and one outgoing edge.

\vspace{.5\baselineskip}
\noindent
\emph{$x \times y$ Grid}. A rectangular lattice $x$ nodes wide by $y$ nodes tall, with $x(y-1) + y(x-1)$ edges. The \textit{shortest distances} benchmark requires all edges to have an integer ``cost'' of traversal. The \textit{grid} host graphs passed to this program have the top-left node marked grey, all edges directed either rightwards or downwards, a cost of one assigned to half of the edges, and a cost of two to the other half.

\subsection{Benchmark performance}\label{sec:benchperf}

%For the sake of brevity we have omitted some intermediate results which followed the general pattern, such as the 4x4 grid in \textit{vertex-colouring}. Where multiple host graphs produced times of less than 0.01 seconds, we include only the largest. Likewise we show only the smallest that exceeded the five minute time limit.

\vspace{.5\baselineskip}
\noindent
\emph{Single-result mode.}
Table \ref{table:resultsSingle} summarises results for the reference interpreter operating in single-result mode. The \textit{Apps} column shows the number of rule applications required to reach the solution. \textit{Time} lists the sum of user and system time reported by the \texttt{time} command. The final two columns show the maximum amount of memory requested by the \texttt{gp2} executable, and the maximum memory holding live data respectively. The disparity between these two numbers, which sometimes approaches a factor of three, results from the Haskell run-time system requesting memory from the operating system in large chunks.

\begin{table}[t]
\begin{minipage}{\textwidth}
\centering

\begin{tabular}{llrrcrr}
\hline 
&  & & & & \multicolumn{2}{c}{Heap/kB}\\
Benchmark          & Host Graph & Apps & Time/s   & & Allocd & Live \\
\hline 
\input{results_table_one}
\end{tabular}

\caption[Reference interpreter benchmarks]{Reference interpreter benchmark results when generating a single output graph}

\label{table:resultsSingle}
\end{minipage}
\end{table}



% In the \textit{acyclicity test} benchmark, notice that the \textit{cyclic $n$} host graphs show rule application counts of zero.  % TODO: is this right?!? Surely a predicate rule has to be applied?

% Notice the rapid increase in live heap usage for the \textit{transitive closure} benchmark, with an increase in the length of the linear host graph by ten increasing memory requirements by approximately an order of magnitude. %todo; discuss

\vspace{.5\baselineskip}
\noindent
\emph{All-result mode.}
Table \ref{table:resultsAll} summarises the performance of the reference interpreter running in all-result mode. This table contains three additional columns showing the total number of output graphs, the number of distinct output graphs up to isomorphism, and the number of executions that terminated in failure.
% either due to a final placed rule failing to match, or an explicit call to GP~2's \texttt{fail} directive.
Where different solutions required differing numbers of rule applications the \textit{Apps} column now shows the range of values.


\begin{table}[t]
\begin{minipage}{\textwidth}
\centering

\begin{tabular}{llrrrrrcrr}
\hline 
&  & \multicolumn{3}{c}{Output Graphs} & & && \multicolumn{2}{c}{Heap/kB}\\
Benchmark          & Host Graph & Total & Unique   & Failed & Apps & Time/s   & & Total  & Live \\
\hline 
\input{results_table_all}
\end{tabular}

\caption[Reference interpreter benchmarks]{Reference interpreter benchmark results when generating all possible output graphs}

\label{table:resultsAll}
\end{minipage}
\end{table}

The extra costs of evaluating a program in all-result mode go beyond those of generating all possible output graphs; the interpreter must also test them for isomorphism. Unsurprisingly, execution time increases sharply with increasing size of host graph, putting many of the computations that completed in single-result mode beyond our five-minute execution-time limit.

The effect on heap usage of producing all possible results is less than one might expect for the \textit{3x3 grid} host graph in both the \textit{acyclicity test} and \textit{shortest distances} programs, given the tens of thousands of isomorphic graphs generated. We benefit from Haskell's lazy evaluation of the list of output graphs.
As there is a single isomorphism class, at most two final host graphs are needed in memory simultaneously --- though there may be many intermediate graphs awaiting further processing.

In contrast, the \textit{vertex colouring} benchmark has many distinct solutions.
% Unfortunately, when \texttt{gp2} is killed by an external process such as the \texttt{timeout} command, the Haskell memory profiler does not write its data to disk before exiting, so we cannot give accurate figures for memory usage for this benchmark, however we noted that ...
As the five minute limit approached during all-results computation for the \textit{3x3 grid} host graph, \texttt{gp2} had been allocated over seven gigabytes, putting a conservative estimate of its live heap in excess of two gigabytes!

\subsection{Discussion}

In single-result mode, performance is acceptable even for some quite complex programs. However, in all-result mode, execution time and memory usage can increase very rapidly with problem size. An extreme example is the vertex-colouring program, which exhibits factorial growth in the number of possible intermediate graphs as edge-counts in initial graphs increase.

The current version of the interpreter uses a finite-map library for indexed sets of nodes and edges in graphs.
Early versions stored these sets as association lists, resulting in an interpreter which spent most of its
execution time traversing lists of nodes and edges.
% Switching to the faster map yielded a factor of two speed improvement. A smaller improvement was realised by the switch to edge keys as a triplet incorporating the unique identifiers of the source and target nodes. Nevertheless, an underlying data structure tuned to our specific usage patterns and judiciously indexed would further reduce the cost of node and edge retrieval.
The cumulative effect of several incremental improvements to our original prototype, without making it larger or
more complicated, was a large speed-up. This in turn enabled us to run larger computations,
putting greater stress on stack and heap memory.  There may yet be quite simple modifications that would reduce
memory demand --- we have made comparatively little effort in this direction.

As discussed in Section \ref{sec:graph-match} the reference interpreter matches nodes and edges in separate passes. This makes for a simple algorithm at the expense of performance. A more performance focussed implementation might use a \textit{search plan}\cite{Horvath-Varro07} in which a graph morphism is built incrementally by adding both nodes and edges to an existing partial morphism, back-tracking if no suitable candidate can be found.

% interpreter cost vs compiler
%\subsubsection*{Interpreter costs}

%Any interpreter has run-time costs which would be paid at compile-time in a compiler. These costs can account for a significant percentage of the execution time.

%In the sub-second execution time benchmarks, profiling information indicated that a significant portion of the execution time was spent parsing the rule-graphs and building map structures.

% We have inevitably incurred some additional costs from the instrumentation required to gather the profiling information reported here.
% TODO: guestimate profiling costs








