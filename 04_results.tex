\section{The Results}\label{the-results}



\begin{table}[h]
\begin{minipage}{\textwidth}
\centering

\begin{tabular}{llcrrrcrrcrr}
\hline 
&  && \multicolumn{3}{c}{Output Graphs} && & && \multicolumn{2}{c}{Heap/kB}\\
Benchmark          & Host Graph\footnotemark & & Total & Unique   & Failed & & Apps & Time/s   & & Total  & Live \\
\hline 
\input{results_table_all}
\end{tabular}

\caption[Reference interpreter benchmarks]{Reference interpreter benchmark results when generating all possible output graphs}

\label{table:resultsAll}
\footnotetext{For a description of host graphs see \ref{TODO:fixme}}
\end{minipage}
\end{table}




\begin{table}[h]
\begin{minipage}{\textwidth}
\centering

\begin{tabular}{llrrcrr}
\hline 
&  & & & & \multicolumn{2}{c}{Heap/kB}\\
Benchmark          & Host Graph\footnotemark & Apps & Time/s   & & Allocd & Live \\
\hline 
\input{results_table_one}
\end{tabular}

\caption[Reference interpreter benchmarks]{Reference interpreter benchmark results when generating a single output graph}

\label{table:resultsSingle}
\footnotetext{For a description of host graphs see \ref{TODO:fixme}}
\end{minipage}
\end{table}




\subsection{Host Graphs}
\label{subsec:hosts}

Host graphs used for benchmarking are named to give an indication of their overall structure.


\paragraph*{Linear $n$}

A chain of $n$ nodes. The first node has only a single outgoing edge. The last node has only a single incoming edge, and all other nodes have exactly one incoming and one outgoing edge.

\paragraph*{Cyclic $n$}

As Linear $n$, but with an extra edge linking the first and final placed nodes so that every node has exactly one incoming and one outgoing edge.

\paragraph*{$x \times y$ Grid}

A square lattice of $x$ nodes wide by $y$ nodes tall.

\paragraph*{Gen $n$}

The Sierpinski program expects a host graph containing a single node with a numeric label, which is used as the number of iterations of the \texttt{expand!} rule to run.

\paragraph*{$n$ node tri grid}

A triangular lattice of $n$ nodes.


Note that some benchmarks require additional information in a host graph which is not reflected in this naming scheme. For example the grids used in the \textit{Shortest path} benchmark have numeric weights assigned to edges.

In all cases the host graphs used can be found in the GP2 version-control repository.
\footnote{https://github.com/UoYCS-plasma/GP2/tree/master/Haskell/Benchmarks} % TODO: should do a github 'release' and link to that version, so that actual versions of benchmarks and host graphs are preserved.



\subsubsection*{Compilation options}

The interpreter was compiled with GHC version 7.6.3 with optimisations and profiling support enabled, using the following command-line:

\begin{verbatim}
$ ghc -O2 -prof -fprof-auto -rtsopts -o gp2 Main.hs
\end{verbatim}

The profiling information generated by the resulting instrumented binary was used to obtain the numbers presented here, and also to inform the discussion of these results in the next section.

\subsubsection*{Execution environment}

All benchmarking was done on a quad-core Intel i7 clocked at 3.4GHz, with 8GB RAM, running 64-bit Ubuntu 14.04 LTS with kernel 3.13.0.

\subsubsection*{Running GP2}

The \textsc{gp2} executable has three mandatory commandline arguments: a GP2 program, a host graph on which to run the program, and a bound on the number of rule applications.

The rule application bound, $k$ can be used to restrict the size of the search space, only allowing output graphs that can be reached in fewer than $k$ graph transformations. As part of its output, the interpreter reports the number of unfinished computations which halted upon reaching the bound.

% explain commandline options to interpreter (e.g. max apps)

Additionally \textsc{gp2} accepts two optional arguments that control the number of output graphs generated and use of the built-in isomorphism checker:

Passing \texttt{--no-iso} on the commandline disables the isomorphism checker. As this can result in a very large number of output graphs, this flag accepts an optional numeric parameter which sets a maximum number of graphs to generate.

Passing \texttt{--one} is equivalent to \texttt{--no-iso 1}.


Benchmarks were run using the following commandline, with execution time limited to 1 hour. The rule application bound was set sufficiently high that the limit is not reached in any of the benchmarks being run.

\begin{verbatim}
$ timeout --foreground 1h \
      gp2 +RTS -p -sgc.prof -RTS $program $hostGraph 1000000
\end{verbatim}

Unfortunately when the interpreter is killed after exceeding the allowed execution time memory profiling information is not written to disk.



Haskell makes heavy use of recursion, and so its runtime system uses a bounds-checked stack which is by default 8 MB in size, which can be modified by passing appropriate compile- or run-time flags. Several programs consumed all of this stack space and terminated with overflow exceptions, as noted in the table. %TODO note in tables -- is this due to a bug in GraphIsomorphism?



% discuss why some progs overflowed and others didn't.


\subsection{Benchmark performance}

Tables \ref{table:resultsSingle} and \ref{table:resultsAll} summarise the reference interpreter performance for the six benchmark programs, each run on several host graphs. Table \ref{table:resultsAll} shows figures for generating all possible results for each program/host-graph pair, while table \ref{table:resultsSingle} shows results when the interpreter is running in single output graph mode.


%It is clear from tables \ref{table:resultsSingle} and \ref{table:resultsAll} that our interpreter is in no way competitive with 


% Generating all possible intermediate graphs

\subsubsection*{Exhaustive search}

When viewing the results in \ref{table:resultsAll}, it should be remembered that the interpreter is computing \textit{all possible} output graphs for a given program/host-graph pair, then consolidating the results based on isomorphism. It is therefore unsurprising that execution time increases exponentially with increasing size of host graph.

However, as already mentioned, more consideration was given to code-correctness and readability than to execution speed or memory consumption.

, and while performance is acceptable for some quite complex programs, it is nevertheless easy to find cases in which performance degrades to an unacceptable degree. A particular example being the Sierpinski triangle benchmark program, which exhibits exponential growth in the number of possible intermediate graphs, and has an execution time in excess of two weeks after only three generations of expansion!

Comparing tables \ref{table:resultsAll} and \ref{table:resultsSingle} gives an indication of the additional costs associated with generating all graphs and isomorphism-checking the results.

One of the most serious limits on the performance of the reference interpreter, which impacts matching of any non-trivial rule, is the naive node-matching strategy. Given a rule with $n$ nodes, select all permutations of $n$ nodes from the host graph, then discard those that fail to meet rootedness, label and edge constraints.

While from the point of view of simplicity this is a reasonable choice for finding all possible outputs for each rule, it is a seriously sub-optimal way of finding a single output graph; for any non-trivial rule and host pair the chances of $n$ nodes selected essentially at random matching a rule is very low, meaning that many permutations need to be inspected for each successful match.

That this strategy is repeated for all of the intermediate graphs generated by the previous rule application explains the combinatorial explosions we see in heavily non-deterministic programs like \textit{Sierpinski} and \textit{Shortest path}.

% interpreter cost vs compiler
\subsubsection*{Interpreter costs}

Any interpreter has run-time costs which would be paid at compile-time in a compiler. These costs can account for a significant percentage of the execution time.

In the sub-second execution time benchmarks, profiling information indicated that a significant portion of the execution time was spent parsing the rule-graphs and building map structures.




\subsubsection*{Generic data structures}

The current version of the interpreter uses a generic lazy map data structure which gives a reasonable balance between cost of search and cost of update. 

% mention performance of graph data structure


The first prototypes of the interpreter stored the graphs as linked-lists of key/value pairs. The result was an interpreter which spent most of its execution time traversing these lists retrieving nodes and edges. Switching to the faster map structure yielded a factor of two speed improvement.

Nevertheless, a single function, \texttt{idLookup} which fetches a node or edge by its unique identifier, still typically accounts for up to 35 percent of our total execution time in several of the listed benchmarks.

An underlying data structure tuned to our specific usage patterns, and judiciously indexed could dramatically reduce this high cost of node and edge retrieval.


% Another significant cost is isomorphism checking. 
\subsubsection*{Isomorphism checking}

Another cost associated with generating all possible output graphs is isomorphism checking. In benchmarks where nondeterminism gives us multiple ways of reaching similar output graphs we want a way of reducing this potentially large set of outputs to those which are truly distinct. A particularly extreme example of this can be seen in the \textit{6 node tri-grid} host graph of the \textit{Shortest paths} benchmark. Here we produced nearly 22~000 isomorphic output graphs.


% Cost of profiling
\subsubsection*{Profiling costs}

We have also incurred some incidental costs by gathering the profiling information used to generate 

% TODO: guestimate profiling costs


% TODO: fail early and fail cheaply!
